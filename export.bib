@ARTICLE{9426433,
  author={Jiang, Shuo and Kang, Peiqi and Song, Xinyu and Lo, Benny P.L. and Shull, Peter B.},
  journal={IEEE Reviews in Biomedical Engineering}, 
  title={Emerging Wearable Interfaces and Algorithms for Hand Gesture Recognition: A Survey}, 
  year={2022},
  volume={15},
  number={},
  pages={85-102},
  keywords={Gesture recognition;Prosthetics;Wrist;Exoskeletons;Training;Stroke (medical condition);Machine learning algorithms;Rehabilitation;human-computer interaction;machine learning;electromyography;forcemyography},
  doi={10.1109/RBME.2021.3078190}}

@ARTICLE{10731943,
  author={Zhang, Chunsheng and Liang, Xu and Fan, Dandan and Chen, Junan and Zhang, Bob and Wu, Baoyuan and Zhang, David},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={HGAIQA: A Novel Hand-Geometry-Aware Image Quality Assessment Framework for Contactless Palmprint Recognition}, 
  year={2024},
  volume={73},
  number={},
  pages={1-13},
  keywords={Image quality;Location awareness;Feature extraction;Fingerprint recognition;Image recognition;Electronic mail;Palmprint recognition;Face recognition;Data science;Accuracy;Biometric recognition;contactless palmprint recognition (PPR);hand geometry;image quality;measurement},
  doi={10.1109/TIM.2024.3485454}}

@ARTICLE{9609102,
  author={Li, Chuankun and Li, Shuai and Gao, Yanbo and Zhang, Xiang and Li, Wanqing},
  journal={IEEE Transactions on Cognitive and Developmental Systems}, 
  title={A Two-Stream Neural Network for Pose-Based Hand Gesture Recognition}, 
  year={2022},
  volume={14},
  number={4},
  pages={1594-1603},
  keywords={Feature extraction;Gesture recognition;Convolutional neural networks;Collaboration;Recurrent neural networks;Data mining;Graph neural networks;Bidirectional independently recurrent neural network (IndRNN);graph convolutional network (GCN);hand gesture recognition},
  doi={10.1109/TCDS.2021.3126637}}

@ARTICLE{9427388,
  author={Gao, Qing and Chen, Yongquan and Ju, Zhaojie and Liang, Yi},
  journal={IEEE Sensors Journal}, 
  title={Dynamic Hand Gesture Recognition Based on 3D Hand Pose Estimation for Human–Robot Interaction}, 
  year={2022},
  volume={22},
  number={18},
  pages={17421-17430},
  keywords={Gesture recognition;Three-dimensional displays;Pose estimation;Feature extraction;Skeleton;Databases;Data gloves;Dynamic hand gestures;hand pose estimation;neural network},
  doi={10.1109/JSEN.2021.3059685}}

@INPROCEEDINGS{9506135,
  author={Yin, Yue and McCarthy, Chris and Rezazadegan, Dana},
  booktitle={2021 IEEE International Conference on Image Processing (ICIP)}, 
  title={Real-Time 3D Hand-Object Pose Estimation for Mobile Devices}, 
  year={2021},
  volume={},
  number={},
  pages={3288-3292},
  keywords={Performance evaluation;Solid modeling;Three-dimensional displays;Image recognition;Computational modeling;Pose estimation;Memory management;3D hand pose estimation;mobile computing;gesture recognition;virtual reality;augmented reality},
  doi={10.1109/ICIP42928.2021.9506135}}


@article{Yuniarno_2024, 
title={Sign Language Recognition Based on Geometric Features Using Deep Learning}, volume={13}, 
url={https://ejournal.undiksha.ac.id/index.php/janapati/article/view/82103}, DOI={10.23887/janapati.v13i2.82103}, abstractNote={&amp;lt;p&amp;gt;Sign language plays a crucial role in facilitating communication among individuals with hearing impairments. In Indonesia, the deaf community often rely on BISINDO (Indonesian Sign Language) to communicate amongst themselves. People who are unfamiliar with sign language will face difficulties. This research aims to develop a system for recognizing sign language using geometric features extracted from hand joint coordinates using Google’s MediaPipe Hands framework. The dataset contains 12 common words, each recorded 30 times with 30 frames recorded for each instance. This will facilitate communication between deaf and hearing individuals. We conducted tests on LSTM- Geometric and CNN1D- Geometric models using geometric features, and on CNN-LSTM-Spatial and CNN1D-LSTM-Spatial models using spatial features. The results indicate that the LSTM model with geometric features achieved the highest accuracy of 99%. Geometric features have been shown to be more effective than spatial features for classifying sign language gestures.&amp;lt;/p&amp;gt;}, number={2}, journal={Jurnal Nasional Pendidikan Teknik Informatika : JANAPATI}, author={Yuniarno, Eko Mulyanto}, year={2024}, month={Jul.}, pages={338–348} }

@article{AVOLA2022108762,
title = {3D hand pose and shape estimation from RGB images for keypoint-based hand gesture recognition},
journal = {Pattern Recognition},
volume = {129},
pages = {108762},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108762},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002436},
author = {Danilo Avola and Luigi Cinque and Alessio Fagioli and Gian Luca Foresti and Adriano Fragomeni and Daniele Pannone},
keywords = {Hand pose estimation, Hand shape estimation, Deep learning, Hand gesture recognition},
abstract = {Estimating the 3D pose of a hand from a 2D image is a well-studied problem and a requirement for several real-life applications such as virtual reality, augmented reality, and hand gesture recognition. Currently, reasonable estimations can be computed from single RGB images, especially when a multi-task learning approach is used to force the system to consider the shape of the hand when its pose is determined. However, depending on the method used to represent the hand, the performance can drop considerably in real-life tasks, suggesting that stable descriptions are required to achieve satisfactory results. In this paper, we present a keypoint-based end-to-end framework for 3D hand and pose estimation and successfully apply it to the task of hand gesture recognition as a study case. Specifically, after a pre-processing step in which the images are normalized, the proposed pipeline uses a multi-task semantic feature extractor generating 2D heatmaps and hand silhouettes from RGB images, a viewpoint encoder to predict the hand and camera view parameters, a stable hand estimator to produce the 3D hand pose and shape, and a loss function to guide all of the components jointly during the learning phase. Tests were performed on a 3D pose and shape estimation benchmark dataset to assess the proposed framework, which obtained state-of-the-art performance. Our system was also evaluated on two hand-gesture recognition benchmark datasets and significantly outperformed other keypoint-based approaches, indicating that it is an effective solution that is able to generate stable 3D estimates for hand pose and shape.}
}

@article{SONG2021103055,
title = {Human pose estimation and its application to action recognition: A survey},
journal = {Journal of Visual Communication and Image Representation},
volume = {76},
pages = {103055},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103055},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000262},
author = {Liangchen Song and Gang Yu and Junsong Yuan and Zicheng Liu},
keywords = {Pose estimation, Action recognition},
abstract = {Human pose estimation aims at predicting the poses of human body parts in images or videos. Since pose motions are often driven by some specific human actions, knowing the body pose of a human is critical for action recognition. This survey focuses on recent progress of human pose estimation and its application to action recognition. We attempt to provide a comprehensive review of recent bottom-up and top-down deep human pose estimation models, as well as how pose estimation systems can be used for action recognition. Thanks to the availability of commodity depth sensors like Kinect and its capability for skeletal tracking, there has been a large body of literature on 3D skeleton-based action recognition, and there are already survey papers such as [1] about this topic. In this survey, we focus on 2D skeleton-based action recognition where the human poses are estimated from regular RGB images instead of depth images. We summarize the performance of recent action recognition methods that use pose estimated from color images as input, then show that there is much room for improvements in this direction.}
}

@article{LIU2024128049,
title = {Deep learning for 3D human pose estimation and mesh recovery: A survey},
journal = {Neurocomputing},
volume = {596},
pages = {128049},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128049},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224008208},
author = {Yang Liu and Changzhen Qiu and Zhiyong Zhang},
keywords = {Human pose estimation, 3D human pose, Human mesh recovery, Human reconstruction, Deep learning, Literature survey},
abstract = {3D human pose estimation and mesh recovery have attracted widespread research interest in many areas, such as computer vision, autonomous driving, and robotics. Deep learning on 3D human pose estimation and mesh recovery has recently thrived, with numerous methods proposed to address different problems in this area. In this paper, to stimulate future research, we present a comprehensive review of recent progress over the past five years in deep learning methods for this area by delving into over 200 references. To the best of our knowledge, this survey is arguably the first to comprehensively cover deep learning methods for 3D human pose estimation, including both single-person and multi-person approaches, as well as human mesh recovery, encompassing methods based on explicit models and implicit representations. We also present comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions. A regularly updated project page can be found at https://github.com/liuyangme/SOTA-3DHPE-HMR.}
}

@article{ZHANG20242399,
title = {Recent Advances on Deep Learning for Sign Language Recognition},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {139},
number = {3},
pages = {2399-2450},
year = {2024},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.045731},
url = {https://www.sciencedirect.com/science/article/pii/S1526149224001279},
author = {Yanqiong Zhang and Xianwei Jiang},
keywords = {Sign language recognition, deep learning, artificial intelligence, computer vision, gesture recognition},
abstract = {Sign language, a visual-gestural language used by the deaf and hard-of-hearing community, plays a crucial role in facilitating communication and promoting inclusivity. Sign language recognition (SLR), the process of automatically recognizing and interpreting sign language gestures, has gained significant attention in recent years due to its potential to bridge the communication gap between the hearing impaired and the hearing world. The emergence and continuous development of deep learning techniques have provided inspiration and momentum for advancing SLR. This paper presents a comprehensive and up-to-date analysis of the advancements, challenges, and opportunities in deep learning-based sign language recognition, focusing on the past five years of research. We explore various aspects of SLR, including sign data acquisition technologies, sign language datasets, evaluation methods, and different types of neural networks. Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) have shown promising results in fingerspelling and isolated sign recognition. However, the continuous nature of sign language poses challenges, leading to the exploration of advanced neural network models such as the Transformer model for continuous sign language recognition (CSLR). Despite significant advancements, several challenges remain in the field of SLR. These challenges include expanding sign language datasets, achieving user independence in recognition systems, exploring different input modalities, effectively fusing features, modeling co-articulation, and improving semantic and syntactic understanding. Additionally, developing lightweight network architectures for mobile applications is crucial for practical implementation. By addressing these challenges, we can further advance the field of deep learning for sign language recognition and improve communication for the hearing-impaired community.}
}